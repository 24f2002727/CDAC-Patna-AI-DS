{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-3rv6-N4q99"
      },
      "outputs": [],
      "source": [
        "#deep learning - 1.using numpy 2.PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.using numpy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#input Data mappings\n",
        "x=np.array([[-2],[-1],[0],[1],[2]])\n",
        "y=np.array([[0],[0],[0],[1],[1]])\n",
        "\n",
        "\n",
        "#----data preprocessed\n",
        "\n",
        "#using sigmoid activation fn\n",
        "def sigmoid(x):return 1/(1+np.exp(-x))\n",
        "#sigmoid derivatives\n",
        "def sigmoid_derivatives(x):return x*(1-x)\n",
        "\n",
        "#initialse the parameters\n",
        "np.random.seed(42)\n",
        "#for neuron1\n",
        "w1=np.random.randn(1,1)   #np.random.randn(1, 1) generates a 1Ã—1 NumPy array containing a single random number drawn from a standard normal (Gaussian) distribution.\n",
        "#here we have taken 2d array to dot use dot product for weighted sum\n",
        "b1=np.zeros((1,1))\n",
        "#for neuron2\n",
        "w2=np.random.randn(1,1)\n",
        "b2=np.zeros((1,1))\n",
        "\n",
        "#training parameters\n",
        "learning_rate=0.1\n",
        "epochs=5000\n",
        "\n",
        "#executing out training loops\n",
        "for epoch in range(epochs):\n",
        "  #forward propagation   --distance from actual or loss\n",
        "\n",
        "  #i/p layer\n",
        "  z1=np.dot(x,w1)+b1 #we are calculating the weighted inputs\n",
        "  a1=sigmoid(z1)\n",
        "\n",
        "  #o/p layer\n",
        "  z2=np.dot(x,w2)+b2\n",
        "  output=sigmoid(z2)\n",
        "\n",
        "  #loss function\n",
        "  loss=np.mean((y-output)**2)\n",
        "\n",
        "  #backward propagation  --learning\n",
        "  #o/p layer\n",
        "  d_output=(y-output)*sigmoid_derivatives(output)   #\n",
        "  d_w2=np.dot(a1.T,d_output)    #here we transposing a1 with a1.T\n",
        "  d_b2=np.sum(d_output,axis=0,keepdims=True)  #here keepdims maintains the dimensions\n",
        "\n",
        "  #hidden layer gradient\n",
        "  d_hidden=np.dot(d_output,w2.T)*sigmoid_derivatives(a1)\n",
        "  d_w1=np.dot(x.T,d_hidden)\n",
        "  d_b1=np.sum(d_hidden,axis=0,keepdims=True)\n",
        "\n",
        "  #now we will be updating the o/p layer weight and bias\n",
        "  w2+=learning_rate*d_w2\n",
        "  b2+=learning_rate*d_b2\n",
        "\n",
        "  #now we will be updating hidden layer weight and bias\n",
        "  w1+=learning_rate*d_w1\n",
        "  b1+=learning_rate*d_b1\n",
        "\n",
        "  #print losses at every 1000 epochs\n",
        "  if epoch %1000==0:\n",
        "    print(f\"Epochs {epoch}, Loss:{loss :.4f}\")\n",
        "\n",
        "#testing our inputs\n",
        "testing_inputs=np.array([[-3],[-.5],[0.75],[3],[1],[0]])\n",
        "\n",
        "#forward pass to test data\n",
        "z1=np.dot(testing_inputs,w1)+b1\n",
        "a1=sigmoid(z1)\n",
        "z2=np.dot(a1,w2)+b2\n",
        "predictions=sigmoid(z2)\n",
        "print(\"Predictions\")\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pee__jhCB600",
        "outputId": "4d658220-d7f5-4bb1-a653-924a67067187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 0, Loss:0.2936\n",
            "Epochs 1000, Loss:0.0183\n",
            "Epochs 2000, Loss:0.0071\n",
            "Epochs 3000, Loss:0.0040\n",
            "Epochs 4000, Loss:0.0027\n",
            "Predictions\n",
            "[[0.07575155]\n",
            " [0.08137866]\n",
            " [0.80654114]\n",
            " [0.93569339]\n",
            " [0.89717416]\n",
            " [0.12876753]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using pyTorch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#input and labels\n",
        "x=torch.tensor([[-2.0],[-1.0],[0.0],[1.0],[2.0]])\n",
        "y=torch.tensor([[0.0],[0.0],[0.0],[1.0],[1.0]])\n",
        "\n",
        "#define neural network\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleNN,self).__init__()\n",
        "    self.hidden=nn.Linear(1,1) #hidden neuron -> w1,b1\n",
        "    self.output=nn.Linear(1,1)  #output neuron -> w2,b2\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    a1=self.sigmoid(self.hidden(x))\n",
        "    out=self.sigmoid(self.output(a1))\n",
        "    return out\n",
        "\n",
        "#model\n",
        "model=SimpleNN()\n",
        "\n",
        "#loss and optimiser\n",
        "criterion=nn.MSELoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=0.1)\n",
        "\n",
        "#training loop\n",
        "epochs=5000\n",
        "for epoch in range(epochs):\n",
        "  #forward pass\n",
        "  output=model(x)\n",
        "  loss=criterion(output,y)\n",
        "\n",
        "  #backpropagation\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%1000==0:\n",
        "    print(f\"Epochs {epoch}, Loss:{loss :.4f}\")\n",
        "\n",
        "#testing our inputs\n",
        "testing_inputs=torch.tensor([[-3],[-.5],[0.5],[3],[1]])\n",
        "\n",
        "# Perform forward pass to get predictions for testing data\n",
        "predictions = model(testing_inputs)\n",
        "\n",
        "print(\"Predictions:\")\n",
        "print(predictions.detach())  #method in PyTorch is used to remove a tensor from the computation graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKUkhLKDG564",
        "outputId": "4bc4abb0-4308-4116-f639-615b2f455076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 0, Loss:0.2536\n",
            "Epochs 1000, Loss:0.0425\n",
            "Epochs 2000, Loss:0.0158\n",
            "Epochs 3000, Loss:0.0088\n",
            "Epochs 4000, Loss:0.0059\n",
            "Predictions:\n",
            "tensor([[0.0315],\n",
            "        [0.0365],\n",
            "        [0.5820],\n",
            "        [0.9381],\n",
            "        [0.9054]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8545917c"
      },
      "source": [
        "## Explanation of the NumPy Implementation (Cell `Pee__jhCB600`)\n",
        "\n",
        "This code implements a simple, single-hidden-layer neural network from scratch using NumPy. It demonstrates the core concepts of a neural network, including forward propagation, loss calculation, backpropagation, and parameter updates.\n",
        "\n",
        "### 1. Data Preparation\n",
        "- **Input and Output Data:** `x` and `y` are defined as NumPy arrays representing simple input features and their corresponding binary labels (0 or 1).\n",
        "\n",
        "### 2. Activation Functions\n",
        "- **`sigmoid(x)`:** This function implements the sigmoid activation, which squashes values between 0 and 1. It's commonly used in the output layer for binary classification or in hidden layers.\n",
        "- **`sigmoid_derivatives(x)`:** This calculates the derivative of the sigmoid function, which is crucial for the backpropagation step. It takes the *output* of the sigmoid function as input.\n",
        "\n",
        "### 3. Model Initialization\n",
        "- **Weights and Biases:** `w1`, `b1` (for the hidden layer) and `w2`, `b2` (for the output layer) are initialized with random values (weights) or zeros (biases). `np.random.seed(42)` ensures reproducibility of the random initialization.\n",
        "- **Training Parameters:** `learning_rate` controls the step size during parameter updates, and `epochs` defines the number of training iterations.\n",
        "\n",
        "### 4. Training Loop (`for epoch in range(epochs):`)\n",
        "\n",
        "#### a. Forward Propagation\n",
        "- **Hidden Layer:**\n",
        "  - `z1 = np.dot(x, w1) + b1`: Calculates the weighted sum of inputs and bias for the hidden layer.\n",
        "  - `a1 = sigmoid(z1)`: Applies the sigmoid activation function to `z1` to get the output of the hidden layer.\n",
        "- **Output Layer:**\n",
        "  - `z2 = np.dot(a1, w2) + b2`: Calculates the weighted sum of hidden layer outputs and bias for the output layer.\n",
        "  - `output = sigmoid(z2)`: Applies the sigmoid activation to `z2` to get the final predictions.\n",
        "\n",
        "#### b. Loss Calculation\n",
        "- **`loss = np.mean((y - output)**2)`:** Calculates the Mean Squared Error (MSE) between the true labels (`y`) and the model's predictions (`output`). This quantifies how far off the predictions are.\n",
        "\n",
        "#### c. Backward Propagation\n",
        "This is where the model learns by adjusting its parameters based on the loss.\n",
        "- **Output Layer Gradients:**\n",
        "  - `d_output = (y - output) * sigmoid_derivatives(output)`: Calculates the error gradient at the output layer. The `(y - output)` part is the difference between target and prediction, and `sigmoid_derivatives(output)` scales this error based on the sigmoid's steepness.\n",
        "  - `d_w2 = np.dot(a1.T, d_output)`: Computes the gradient for the output layer weights (`w2`) using the transpose of the hidden layer's output (`a1.T`).\n",
        "  - `d_b2 = np.sum(d_output, axis=0, keepdims=True)`: Computes the gradient for the output layer bias (`b2`).\n",
        "- **Hidden Layer Gradients:**\n",
        "  - `d_hidden = np.dot(d_output, w2.T) * sigmoid_derivatives(a1)`: Backpropagates the error from the output layer to the hidden layer, considering the output weights (`w2.T`) and the hidden layer's sigmoid derivative.\n",
        "  - `d_w1 = np.dot(x.T, d_hidden)`: Computes the gradient for the hidden layer weights (`w1`).\n",
        "  - `d_b1 = np.sum(d_hidden, axis=0, keepdims=True)`: Computes the gradient for the hidden layer bias (`b1`).\n",
        "\n",
        "#### d. Parameter Updates\n",
        "- `w2 += learning_rate * d_w2`\n",
        "- `b2 += learning_rate * d_b2`\n",
        "- `w1 += learning_rate * d_w1`\n",
        "- `b1 += learning_rate * d_b1`:\n",
        "  The weights and biases are updated by subtracting a fraction (`learning_rate`) of their respective gradients. This moves the parameters in the direction that reduces the loss.\n",
        "\n",
        "### 5. Testing\n",
        "- **`testing_inputs`:** A new set of inputs is defined to evaluate the trained model.\n",
        "- **Forward Pass for Testing:** The `testing_inputs` are passed through the *trained* network (using the updated `w1, b1, w2, b2`) to generate `predictions`.\n",
        "- The `predictions` are then printed, showing the model's output for unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076c83ce"
      },
      "source": [
        "## Explanation of the PyTorch Implementation (Cell `tKUkhLKDG564`)\n",
        "\n",
        "This code implements the same simple, single-hidden-layer neural network using the PyTorch framework. PyTorch simplifies the process by providing modules for layers, loss functions, and optimizers, automating much of the gradient calculation (autograd).\n",
        "\n",
        "### 1. Imports\n",
        "- **`torch`**: The main PyTorch library.\n",
        "- **`torch.nn` (neural network)**: Contains modules for building neural network layers (e.g., `Linear`, `Sigmoid`).\n",
        "- **`torch.optim` (optimizer)**: Provides optimization algorithms (e.g., `SGD`).\n",
        "\n",
        "### 2. Data Preparation\n",
        "- **Input and Output Data:** `x` and `y` are defined as `torch.tensor` objects, which are PyTorch's equivalent of NumPy arrays, but they can track gradients.\n",
        "\n",
        "### 3. Define Neural Network (`SimpleNN` class)\n",
        "- **Inherits `nn.Module`**: All PyTorch neural networks are defined as classes that inherit from `nn.Module`. This provides the basic functionality for tracking parameters and submodules.\n",
        "- **`__init__(self)` constructor:**\n",
        "  - `super(SimpleNN, self).__init__()`: Calls the constructor of the parent `nn.Module` class.\n",
        "  - `self.hidden = nn.Linear(1, 1)`: Defines the hidden layer. `nn.Linear(in_features, out_features)` creates a linear transformation (weight multiplication + bias addition). Here, 1 input feature maps to 1 output feature (neuron).\n",
        "  - `self.output = nn.Linear(1, 1)`: Defines the output layer, also a linear transformation.\n",
        "  - `self.sigmoid = nn.Sigmoid()`: Initializes the sigmoid activation function.\n",
        "- **`forward(self, x)` method:**\n",
        "  - This method defines how data flows through the network.\n",
        "  - `a1 = self.sigmoid(self.hidden(x))`: The input `x` passes through the hidden linear layer, and then the sigmoid activation is applied.\n",
        "  - `out = self.sigmoid(self.output(a1))`: The output of the hidden layer (`a1`) passes through the output linear layer, followed by another sigmoid activation to produce the final `out`put.\n",
        "\n",
        "### 4. Model, Loss, and Optimizer Initialization\n",
        "- **`model = SimpleNN()`**: Creates an instance of our neural network.\n",
        "- **`criterion = nn.MSELoss()`**: Defines the Mean Squared Error (MSE) loss function. This is equivalent to `np.mean((y - output)**2)` in the NumPy example.\n",
        "- **`optimizer = optim.SGD(model.parameters(), lr=0.1)`**: Initializes the Stochastic Gradient Descent (SGD) optimizer. It takes `model.parameters()` (which are all the trainable weights and biases defined in `nn.Linear` layers) and the `learning_rate` as arguments. This optimizer will handle updating the parameters based on gradients.\n",
        "\n",
        "### 5. Training Loop (`for epoch in range(epochs):`)\n",
        "\n",
        "#### a. Forward Pass\n",
        "- `output = model(x)`: Feeds the input data `x` through the `model`. This implicitly calls the `forward` method of the `SimpleNN` class.\n",
        "- `loss = criterion(output, y)`: Calculates the loss between the model's `output` and the true `y` using the defined `MSELoss` criterion.\n",
        "\n",
        "#### b. Backpropagation and Parameter Update\n",
        "- **`optimizer.zero_grad()`**: Clears the gradients of all optimized `torch.Tensor`s. It's crucial to do this at the start of each epoch, as PyTorch accumulates gradients by default.\n",
        "- **`loss.backward()`**: This is the magic of PyTorch's autograd. It automatically computes the gradients of the `loss` with respect to all parameters that have `requires_grad=True` (which `nn.Linear` layers automatically do).\n",
        "- **`optimizer.step()`**: Performs a single optimization step (parameter update) based on the computed gradients and the `learning_rate`. This updates the model's weights and biases.\n",
        "\n",
        "### 6. Testing\n",
        "- **`testing_inputs = torch.tensor([...])`**: A new set of inputs is defined as a PyTorch tensor.\n",
        "- **`predictions = model(testing_inputs)`**: The trained `model` is used to make predictions on the `testing_inputs`.\n",
        "- **`print(predictions.detach())`**: The `.detach()` method is used here to remove the `predictions` tensor from the computation graph. This is good practice when you just want to view or use the tensor's value without needing to track its gradients further (e.g., for printing or converting to NumPy). It prevents unnecessary memory usage and computations related to gradients."
      ]
    }
  ]
}