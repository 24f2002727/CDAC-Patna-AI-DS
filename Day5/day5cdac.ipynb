{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBmAiRul8eh0"
      },
      "outputs": [],
      "source": [
        "#1.importingg important libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "xugQ0xUE9DJq",
        "outputId": "65452a34-8a1b-4383-d56c-b2cf54cccede"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2375967042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m data={\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m'Age'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m'Salary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m'Gender'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'female'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'female'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'female'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# from numpy._core.defchararray import upper\n",
        "#2.creating a raw dataset\n",
        "\n",
        "data={\n",
        "    'Age':[25,30,np.nan,45,50],\n",
        "    'Salary':[30000,40000,50000,60000,70000],\n",
        "    'Gender':['male','female','female','male','female']\n",
        "}\n",
        "\n",
        "#storing data to tables\n",
        "df=pd.DataFrame(data)\n",
        "print(\"Data stored\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "\n",
        "#3.data preprocessing\n",
        "\n",
        "#3.1.updating the missing values\n",
        "df['Age'].fillna(df['Age'].mean(),inplace=True)\n",
        "print(\"Missing values updated\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "\n",
        "#3.2. encoding data\n",
        "  #->method1-\n",
        "df=pd.get_dummies(df,columns=['Gender'], dtype=int)  #it make dummy columns\n",
        "print(\"One hot encoded data\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "   #->method2-one hot encoding\n",
        "df=pd.get_dummies(df,columns=['Gender'], dtype=int)  #it make dummy columns\n",
        "print(\"One hot encoded data\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "\n",
        "#3.3 Scaling technique\n",
        "  #->method1-Normalisation technique\n",
        "scaler=MinMaxScaler()\n",
        "df[['Age','Salary']]=scaler.fit_transform(df[['Age','Salary']])\n",
        "print(\"Scaled data normalised\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "  #->method2-Standarisatiion technique\n",
        "std_scaler=StandardScaler()\n",
        "df[['Age','Salary']]=std_scaler.fit_transform(df[['Age','Salary']])\n",
        "print(\"Scaled data standardised\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "#3.4 Feature transformation\n",
        "df['salary_log']=np.log(df['Salary']+1)\n",
        "print(\"Feature transformation\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "#3.4 Handeling outliers-winsorization\n",
        "upper_limit=df['Salary'].quantile(0.95) #it will neglect the top 5% data so that outliers lieing in that range will be removed\n",
        "df['Salary']=np.where(df['Salary']>upper_limit,upper_limit,df['Salary']) #np.where(condn,x,y) An array-like object of booleans. Where condition is True, x is chosen; where False, y is chosen.\n",
        "print(\"Outliers removed\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "#3.5 Bining\n",
        "df['Age_group']=pd.cut(df['Age'],bins=3,labels=['Young', 'Adult', 'Old'])\n",
        "print(\"Bined data\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "#3.6 Feature construction\n",
        "df['Age_to_salary']=df['Age']/(df['Salary']+1)\n",
        "print(\"Feature constructed Age to day\")\n",
        "print(df,\"\\n\")\n",
        "\n",
        "#3.7 Dimensionality reduction\n",
        "pca=PCA(n_components=2)\n",
        "reduced_data=pca.fit_transform(df[['Age','Salary']])\n",
        "print(\"Reduced data\")\n",
        "print(reduced_data,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1caadf"
      },
      "source": [
        "### Explanation of `np.where()`\n",
        "\n",
        "`numpy.where()` is a function in the NumPy library that returns elements chosen from `x` or `y` depending on `condition`. It's incredibly useful for conditional logic within NumPy arrays, similar to an if-else statement but vectorized.\n",
        "\n",
        "**Syntax:**\n",
        "`numpy.where(condition, x, y)`\n",
        "\n",
        "*   **`condition`**: An array-like object of booleans. Where `condition` is `True`, `x` is chosen; where `False`, `y` is chosen.\n",
        "*   **`x`**: Values from which to choose when `condition` is `True`.\n",
        "*   **`y`**: Values from which to choose when `condition` is `False`.\n",
        "\n",
        "**How it was used in your notebook for handling outliers:**\n",
        "\n",
        "In your notebook, `np.where()` was used in the line:\n",
        "`df['Salary']=np.where(df['Salary']>upper_limit,upper_limit,df['Salary'])`\n",
        "\n",
        "Let's break this down:\n",
        "\n",
        "*   **`condition`**: `df['Salary'] > upper_limit`\n",
        "    *   This condition checks, for each value in the 'Salary' column, if that value is greater than the calculated `upper_limit` (which was the 95th percentile of the 'Salary' column).\n",
        "\n",
        "*   **`x`**: `upper_limit`\n",
        "    *   If the condition is `True` (i.e., a salary is greater than the `upper_limit`), then that salary value is replaced with the `upper_limit`. This effectively 'caps' or 'winsorizes' the outliers at the upper end.\n",
        "\n",
        "*   **`y`**: `df['Salary']`\n",
        "    *   If the condition is `False` (i.e., a salary is not greater than the `upper_limit`), then the original salary value is retained.\n",
        "\n",
        "**In summary:** This line of code replaces all salary values that are above the 95th percentile with the 95th percentile value itself, thus mitigating the impact of extreme high outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7757a420"
      },
      "source": [
        "### Standardization vs. Normalization\n",
        "\n",
        "Both standardization and normalization are common data preprocessing techniques used to scale numerical features, but they achieve this in different ways and are suitable for different scenarios.\n",
        "\n",
        "**1. Normalization (Min-Max Scaling)**\n",
        "\n",
        "*   **Goal:** To scale features to a fixed range, usually between 0 and 1.\n",
        "*   **Formula:** `X_scaled = (X - X_min) / (X_max - X_min)`\n",
        "    *   `X`: The original feature value.\n",
        "    *   `X_min`: The minimum value of the feature.\n",
        "    *   `X_max`: The maximum value of the feature.\n",
        "*   **Characteristics:**\n",
        "    *   Transforms data to a specific, bounded range.\n",
        "    *   Sensitive to outliers, as they will compress the range of the majority of the data.\n",
        "*   **When to use:**\n",
        "    *   When you know that the distribution of your data does not follow a Gaussian distribution.\n",
        "    *   Algorithms that require input features to be within a specific range (e.g., neural networks with sigmoid activation functions, k-nearest neighbors).\n",
        "\n",
        "**2. Standardization (Z-score Normalization)**\n",
        "\n",
        "*   **Goal:** To scale features such that they have a mean of 0 and a standard deviation of 1.\n",
        "*   **Formula:** `X_scaled = (X - μ) / σ`\n",
        "    *   `X`: The original feature value.\n",
        "    *   `μ`: The mean of the feature.\n",
        "    *   `σ`: The standard deviation of the feature.\n",
        "*   **Characteristics:**\n",
        "    *   Does not bound values to a specific range, but it makes them unit-less.\n",
        "    *   Less affected by outliers than min-max scaling because it uses the mean and standard deviation, which are robust to some extent.\n",
        "*   **When to use:**\n",
        "    *   When the data follows a Gaussian (bell curve) distribution.\n",
        "    *   Algorithms that assume a Gaussian distribution or are sensitive to the scale of features (e.g., Linear Regression, Logistic Regression, Support Vector Machines, K-Means Clustering, Principal Component Analysis).\n",
        "\n",
        "In your notebook, you applied both:\n",
        "\n",
        "*   **Normalization** using `MinMaxScaler()`: `df[['Age','Salary']]=scaler.fit_transform(df[['Age','Salary']])`\n",
        "*   **Standardization** using `StandardScaler()`: `df[['Age','Salary']]=std_scaler.fit_transform(df[['Age','Salary']])`\n",
        "\n",
        "Note that applying both sequentially means the last one applied (Standardization in this case) is the one that determines the final scaling of the `Age` and `Salary` columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9f6a0e5"
      },
      "source": [
        "### Explanation of One-Hot Encoding\n",
        "\n",
        "One-hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. If a categorical feature has `n` unique values, one-hot encoding will transform this feature into `n` new features (dummy variables), each representing one unique value. Each of these new features will have a value of `1` for the row where the original feature had that category, and `0` otherwise.\n",
        "\n",
        "In our notebook, for the `Gender` column, which contained 'male' and 'female':\n",
        "\n",
        "1. **`LabelEncoder()`** was initially used, which would assign `0` and `1` to 'male' and 'female' respectively. However, this creates an ordinal relationship that might not exist.\n",
        "\n",
        "2. **`pd.get_dummies(df, columns=['Gender'])`** then correctly applied one-hot encoding. It created two new columns: `Gender_female` and `Gender_male`. For each row, one of these columns will have a `1` and the other a `0`, indicating the gender.\n",
        "\n",
        "This is evident in the `df` DataFrame's state, where `Gender_0` and `Gender_1` likely correspond to the encoded 'female' and 'male' categories, respectively, after `get_dummies` was applied on the `LabelEncoder` output."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}